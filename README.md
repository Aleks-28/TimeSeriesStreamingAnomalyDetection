# Streaming Multivariate Anomaly Detection Benchmark

This repository contains the code and resources for a **streaming anomaly detection benchmark** developed during my internship at **EDF R&D**. The benchmark evaluates different models for detecting anomalies in multivariate time-series data under streaming constraints.

---

## ðŸ“¦ Requirements

- `g++` compiler or C++ build tools (Note: the library is **untested on Windows**)
- [Anaconda](https://www.anaconda.com/) or Miniconda (mandatory to use provided environment scripts)

---

## ðŸ›  Installation

To set up the required environments, run the following script:

```bash
scripts/install_env.sh
```

This will install three conda environments:
- One for plotting
- One for the `dSalmon` C++ library
- One for Online Isolation Forest

**Data Download:**  
Instructions for downloading the datasets are available in `utils/data_loaders/`.

---

## ðŸ“ Repository Structure

```text
.
â”œâ”€â”€ images/              # Generated plots and result visualizations
â”œâ”€â”€ notebooks/           # Jupyter notebooks for analysis (see `result_analysis.ipynb`)
â”œâ”€â”€ scripts/             # Job scheduling, log analysis, and batch processing scripts
â”œâ”€â”€ src/                 # Core benchmarking framework (main loop in `runner.py`)
â”œâ”€â”€ utils/               # Utilities including dataset loaders
â”œâ”€â”€ run.py               # Main entry point for running experiments
â”œâ”€â”€ run_multiproc.py     # Script for launching parallel jobs
â””â”€â”€ README.md            # You're here!
```

---

## ðŸš€ Running the Benchmark

Use the following command to run the benchmark:

```bash
python run.py --runs 10 --model SWKNN --dataset_name comut4 --observation_period 100 --sliding_window_factor 0.01
```

---

## ðŸ§µ Running Parallel Experiments

To launch experiments in parallel:

1. **Generate jobs**:
   ```bash
   python scripts/generate_jobs.py
   ```

2. **Run the jobs** using multiprocessing:
   ```bash
   python run_multiproc.py
   ```

---

## âž• Adding a New Model

To include a new model:

1. Create a new Python file in `src/models/` following the template:
   ```bash
   src/templates/model.py
   ```
2. Register the model in `src/models/__init__.py`:
   ```python
   from .MyNewModel import MyNewModel
   ```

3. Use the class name as the `--model` argument in `run.py`.

> **Note:** The model class name is **case-sensitive**.

---

## ðŸ“Š Adding a New Metric

To define a new evaluation metric:

1. Create a new file in `src/metrics/` based on:
   ```bash
   src/templates/metric.py
   ```

2. Add your metric to the `all` list in `src/metrics/__init__.py`:
   ```python
   all = ['AUCPR', 'AUCROC', 'VUSPR', 'VUSROC', 'RefMetrics', 'MyNewMetric']
   ```